"""
Python code extracted from DL_Finalcode.ipynb
Generated by Project Publisher

Original notebook: 42 cells
Code cells: 15
"""

# Imports
from IPython import get_ipython  # Import to interact with the IPython kernel.
from IPython.display import display  # Utility to display widgets in Jupyter notebooks.
from PIL import Image
from cv2 import VideoCapture, imencode
from cv2 import VideoCapture, imencode  # Specific imports from OpenCV for video capture and image encoding
from cv2 import imencode  # Function to encode image formats
from dotenv import load_dotenv
from dotenv import load_dotenv  # Load environment variables from a .env file
from dotenv import load_dotenv  # Module for loading environment variables from a .env file
from fastapi import FastAPI, HTTPException
from fastapi import File, UploadFile
from gtts import gTTS
from ipywidgets import HTML  # HTML widget for displaying rich text or HTML.
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  # LangChain modules for creating prompts and handling message placeholders
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  # Prompt utilities from LangChain
from langchain.schema.messages import SystemMessage  # Module for defining system messages in a chat
from langchain.schema.messages import SystemMessage  # Schema for system messages
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory  # Manages chat histories
from langchain_community.chat_message_histories import ChatMessageHistory  # Module for managing chat history
from langchain_core.output_parsers import StrOutputParser  # Parser for outputting strings from the chat model
from langchain_core.output_parsers import StrOutputParser  # Parses output into strings
from langchain_core.runnables.history import RunnableWithMessageHistory  # Handles message history in chains
from langchain_core.runnables.history import RunnableWithMessageHistory  # Runnable module with message history handling
from langchain_google_genai import ChatGoogleGenerativeAI  # Google GenAI's Chat API for LangChain integration
from langchain_google_genai import ChatGoogleGenerativeAI  # LangChain's Google Generative AI module
from langchain_openai import ChatOpenAI  # LangChain's OpenAI module for using OpenAI's chat models
from langchain_openai import ChatOpenAI  # OpenAI's Chat API for LangChain integration
from livekit import agents, rtc
from livekit.agents import JobContext, WorkerOptions, cli, tokenize, tts
from livekit.agents.llm import ChatContext, ChatImage, ChatMessage
from livekit.agents.voice_assistant import VoiceAssistant
from livekit.plugins import deepgram, openai, silero
from pyaudio import PyAudio, paInt16  # PyAudio modules for handling audio input/output
from pydantic import BaseModel
from speech_recognition import Microphone, Recognizer, UnknownValueError
from speech_recognition import Microphone, Recognizer, UnknownValueError  # Modules for speech recognition and microphone handling
from threading import Lock, Thread
from threading import Lock, Thread  # Modules for threading and thread-safe operations
from transformers import BitsAndBytesConfig
from transformers import pipeline
from transformers import pipeline  # Hugging Face's pipeline for various ML models
from typing import Annotated
from typing import Any
import asyncio
import base64
import base64  # For encoding and decoding data in Base64
import base64  # Module for encoding and decoding binary data into ASCII
import cv2
import cv2  # OpenCV library for image processing
import cv2  # OpenCV library for video capture and image processing
import gradio as gr  # Gradio library for creating web interfaces
import io
import json  # Provides functions to parse and work with JSON data.
import numpy as np  # NumPy for numerical operations
import openai  # OpenAI API client
import openai  # OpenAI library for interacting with OpenAI's models
import pygame
import re
import requests
import subprocess  # Allows running shell commands from within Python.
import sys  # Provides access to some variables used or maintained by the Python interpreter.
import time  # Provides time-related functions, like sleeping for a number of seconds.
import time  # Provides time-related functions, such as sleeping for a number of seconds.
import torch

# Main code
# Library to support asynchronous programming, allowing for concurrent operations.
# Used for adding metadata to type hints.
# Module to load environment variables from a .env file.
# Import necessary modules from the livekit package.
# Load environment variables (e.g., API keys, config settings) from a .env file.
load_dotenv()
# Define a class to manage functions that the AI assistant can invoke.
class FunctionalityContext(agents.llm.FunctionContext):
    """
    This class encapsulates specific functions that the AI assistant can call upon.
    These functions allow the assistant to perform tasks that require additional capabilities,
    such as visual processing.
    """
    # Decorator to mark the following method as callable by the AI when needed.
    @agents.llm.ai_callable(
        description=(
            "This function is invoked when the assistant needs to process visual information."
            "This could involve analyzing an image, a video, or live feed from a webcam."
        )
    )
    async def evaluate_image(
        self,
        triggered_msg: Annotated[
            str,
            agents.llm.TypeInfo(
                description="The message from the user that caused this function to be called."
            ),
        ],
    ):
        # Print a message to indicate that a vision-related task was triggered.
        print(f"Message initiating visual evaluation: {triggered_msg}")
        # Return None since this is just a placeholder for vision processing.
        return None
# Asynchronously obtain the first available video track from a room.
async def obtain_video_feed(room_instance: rtc.Room):
    """
    This function identifies and retrieves the first available video track within the given room.
    The video track will later be used to capture and process images or video frames.
    """
    # Create a future object that will eventually hold the video track.
    video_feed = asyncio.Future[rtc.RemoteVideoTrack]()
    # Iterate over all remote participants (users) in the room.
    for _, participant in room_instance.remote_participants.items():
        # Iterate over all track publications (media streams) associated with the participant.
        for _, track_data in participant.track_publications.items():
            # Check if the current track is a video track.
            if track_data.track is not None and isinstance(
                track_data.track, rtc.RemoteVideoTrack
            ):
                # If a video track is found, set it as the result for the future object.
                video_feed.set_result(track_data.track)
                # Log which video track is being used.
                print(f"Utilizing video track with ID: {track_data.track.sid}")
                # Break the loop as we have found a video track to work with.
                break
    # Return the video track once it is available.
    return await video_feed
# The main function serving as the entry point for the application.
async def start_program(ctx: JobContext):
    # Establish a connection to the room specified in the JobContext.
    await ctx.connect()
    # Print the name of the room to which the connection was established.
    print(f"Connected to room: {ctx.room.name}")
    # Create a new chat context to store the conversation history and initial assistant behavior.
    dialogue_context = ChatContext(
        messages=[
            ChatMessage(
                role="system",  # The system role sets the assistant's personality and guidelines.
                content=(
                    "Your name is Nova. You are a humorous, sharp-witted bot with voice and vision capabilities."
                    "Provide brief, concise responses. Avoid complex punctuation or emojis."
                    # The assistant is instructed to keep responses simple and avoid unpronounceable characters.
                ),
            )
        ]
    )
    # Initialize the language model (LLM) with GPT-4. This model will handle natural language processing.
    model_instance = openai.LLM(model="gpt-4o")
    # Adapt OpenAI's Text-to-Speech (TTS) service for streaming to make it compatible with the voice assistant.
    tts_adapter = tts.StreamAdapter(
        tts=openai.TTS(voice="nova"),  # Specify the voice to be used for TTS.
        sentence_tokenizer=tokenize.basic.SentenceTokenizer(),  # Tokenize sentences for proper streaming.
    )
    # Initialize a variable to hold the latest captured video frame.
    latest_frame: rtc.VideoFrame | None = None
    # Create an instance of the voice assistant with the necessary components.
    nova_assistant = VoiceAssistant(
        vad=silero.VAD.load(),  # Load the Voice Activity Detector (VAD) to detect when the user is speaking.
        stt=deepgram.STT(),  # Load the Speech-to-Text (STT) engine to convert spoken words into text.
        llm=model_instance,  # Use the previously defined GPT-4 language model.
        tts=tts_adapter,  # Use the adapted Text-to-Speech engine for audio responses.
        fnc_ctx=FunctionalityContext(),  # Assign the context that includes callable functions.
        chat_ctx=dialogue_context,  # Use the chat context to maintain conversation history.
    )
    # Create a manager to handle text-based chat messages in the room.
    chat_manager = rtc.ChatManager(ctx.room)
    # Define an asynchronous function to respond to user messages.
    async def respond_to_user(reply_text: str, include_image: bool = False):
        """
        This function responds to the user's message with the provided text.
        Optionally, it can include the most recent image from the video feed in the response.
        """
        # Initialize the response content with the text reply.
        response_content: list[str | ChatImage] = [reply_text]
        # If an image is requested and available, include it in the response.
        if include_image and latest_frame:
            response_content.append(ChatImage(image=latest_frame))
        # Add the user's response to the chat history.
        dialogue_context.messages.append(ChatMessage(role="user", content=response_content))
        # Generate the assistant's response using the language model.
        response_stream = model_instance.chat(chat_ctx=dialogue_context)
        # Have the assistant speak the response aloud, allowing for interruptions if necessary.
        await nova_assistant.say(response_stream, allow_interruptions=True)
    # Event handler triggered when a new chat message is received from the user.
    @chat_manager.on("message_received")
    def handle_received_message(msg: rtc.ChatMessage):
        """
        This function is automatically called whenever a new message is received from the user.
        It processes the message and triggers an appropriate response.
        """
        if msg.message:
            # Asynchronously respond to the user without including an image.
            asyncio.create_task(respond_to_user(msg.message, include_image=False))
    # Event handler triggered after the assistant completes a function call.
    @nova_assistant.on("function_calls_finished")
    def handle_function_completion(called_funcs: list[agents.llm.CalledFunction]):
        """
        This function is automatically called when the assistant completes a function.
        It determines whether the function call was related to image processing.
        """
        if len(called_funcs) == 0:
            return  # If no functions were called, do nothing.
        # Retrieve the original user message that triggered the function call.
        user_message = called_funcs[0].call_info.arguments.get("triggered_msg")
        if user_message:
            # If a message exists, respond to the user and include an image if necessary.
            asyncio.create_task(respond_to_user(user_message, include_image=True))
    # Start the voice assistant and begin processing the room's activity.
    nova_assistant.start(ctx.room)
    # Provide an initial greeting to the user.
    await asyncio.sleep(1)
    await nova_assistant.say("Hello! How can I assist you today?", allow_interruptions=True)
    # Continuously monitor the room's connection state to remain active while connected.
    while ctx.room.connection_state == rtc.ConnectionState.CONN_CONNECTED:
        # Obtain the first available video feed from the room.
        video_feed = await obtain_video_feed(ctx.room)
        # Asynchronously process incoming video frames from the video feed.
        async for stream_event in rtc.VideoStream(video_feed):
            # Continuously update the latest captured frame from the video stream.
            latest_frame = stream_event.frame
# The entry point for running the application.
if __name__ == "__main__":
    # Start the CLI application with the main function as the entry point.
    cli.run_app(WorkerOptions(entrypoint_fnc=start_program))
# Import necessary libraries
# Load environment variables from a .env file (if any)
load_dotenv()
# Define the Assistant class for handling the chat assistant's functionality
class ChatbotAssistant:
    def __init__(self, ai_model):
        # Initialize the assistant with a language model and create the inference pipeline
        self.chat_pipeline = self._initialize_inference_pipeline(ai_model)
    def generate_response(self, user_query, user_image):
        # If the user query is empty, return without processing
        if not user_query:
            return
        # Log the user prompt for debugging purposes
        print("User Query:", user_query)
        # Invoke the inference pipeline with the user's query and the Base64-encoded image
        ai_response = self.chat_pipeline.invoke(
            {"prompt": user_query, "image_base64": user_image.decode()},
            config={"configurable": {"session_id": "unused"}},
        ).strip()
        # Log the AI's response
        print("AI Response:", ai_response)
        # Return the AI's response
        return ai_response
    def _initialize_inference_pipeline(self, ai_model):
        # Define the system's prompt template for guiding the AI's behavior
        SYSTEM_PROMPT_TEMPLATE = """
        You are a witty assistant that will use the chat history and the image
        provided by the user to answer its questions.
        Use few words on your answers. Go straight to the point. Do not use any
        emoticons or emojis. Do not ask the user any questions.
        Be friendly and helpful. Show some personality. Do not be too formal.
        """
        # Create a chat prompt template using the system prompt and placeholders for chat history
        prompt_structure = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=SYSTEM_PROMPT_TEMPLATE),
                MessagesPlaceholder(variable_name="chat_history"),
                (
                    "human",
                    [
                        {"type": "text", "text": "{prompt}"},
                        {
                            "type": "image_url",
                            "image_url": "data:image/jpeg;base64,{image_base64}",
                        },
                    ],
                ),
            ]
        )
        # Create the chain by combining the prompt structure with the AI model and output parser
        response_chain = prompt_structure | ai_model | StrOutputParser()
        # Initialize chat message history for keeping track of the conversation
        conversation_history = ChatMessageHistory()
        # Return a runnable chain with the message history incorporated
        return RunnableWithMessageHistory(
            response_chain,
            lambda _: conversation_history,
            input_messages_key="prompt",
            history_messages_key="chat_history",
        )
# Initialize the ChatbotAssistant with the desired AI model
ai_model_instance = ChatOpenAI(model="gpt-4o-mini")
chatbot = ChatbotAssistant(ai_model_instance)
# Initialize the speech recognition pipeline using Whisper model
audio_transcription = pipeline("automatic-speech-recognition", model="openai/whisper-base.en", device="cuda")
# Initialize an empty stream for storing audio data
audio_stream = None
def process_audio_stream(audio_stream, audio_chunk):
    # Extract the sampling rate and audio data from the new audio chunk
    sample_rate, audio_data = audio_chunk
    # Normalize the audio data
    audio_data = audio_data.astype(np.float32)
    audio_data /= np.max(np.abs(audio_data))
    # Concatenate the new audio chunk to the existing stream
    if audio_stream is not None:
        audio_stream = np.concatenate([audio_stream, audio_data])
    else:
        audio_stream = audio_data
    # Transcribe the audio data using the speech recognition pipeline
    transcription_result = audio_transcription({"sampling_rate": sample_rate, "raw": audio_stream})["text"]
    # Return the updated audio stream and the transcribed text
    return audio_stream, transcription_result
def main_interaction_function(audio_stream, audio_chunk, webcam_image):
    # Wait until the image from the webcam is available
    while webcam_image is None:
        pass
    print("Entered main function")
    # Transcribe the latest audio chunk using the speech recognition pipeline
    transcription_output = process_audio_stream(audio_stream, audio_chunk)
    # Encode the webcam image to JPEG format and then to Base64
    _, image_buffer = imencode(".jpeg", webcam_image)
    encoded_image = base64.b64encode(image_buffer)
    # Initialize the assistant with the AI model
    ai_model_instance = ChatOpenAI(model="gpt-4o-mini")
    chatbot = ChatbotAssistant(ai_model_instance)
    # Generate the final response using the assistant, based on the transcribed text and image
    final_response = chatbot.generate_response(transcription_output[1], encoded_image)
    print(transcription_output)
    # Return the stream and the AI's response
    if isinstance(final_response, list):
        final_response = [audio_stream, final_response[0]]
    elif isinstance(final_response, str):
        return [audio_stream, final_response]
    else:
        raise ValueError(f"Unexpected type: {type(final_response)}, answer: {final_response}")
# Set up the Gradio interface for user interaction
demo_interface = gr.Interface(
    main_interaction_function,
    # ["state", gr.Audio(sources=["microphone"], streaming=True), gr.Image(sources=["webcam"], streaming=True)],
    ["state", gr.Audio(sources=["microphone"]), gr.Image(sources=["webcam"])],
    ["state", "text"],
)
# Launch the Gradio interface with debugging enabled and sharing option active
demo_interface.launch(debug=True, share=True)
# Load environment variables from the .env file
load_dotenv()
# Class to manage the webcam feed
class CameraStream:
    def __init__(self):
        # Initialize the video capture from the webcam
        self.cam = VideoCapture(index=0)
        _, self.current_frame = self.cam.read()  # Read the initial frame
        self.is_active = False  # Flag to check if the stream is running
        self.frame_lock = Lock()  # Lock to ensure thread-safe operations
    # Start the webcam stream
    def begin(self):
        if self.is_active:
            return self
        self.is_active = True  # Set the active flag to true
        # Start the thread to capture video frames
        self.stream_thread = Thread(target=self._update_frame, args=())
        self.stream_thread.start()
        return self
    # Update the frame continuously in a separate thread
    def _update_frame(self):
        while self.is_active:
            _, frame = self.cam.read()  # Capture the frame from the webcam
            with self.frame_lock:  # Ensure thread-safe operation
                self.current_frame = frame
    # Read the current frame, with an option to encode it in base64
    def capture_frame(self, encode=False):
        with self.frame_lock:
            frame = self.current_frame.copy()
        if encode:  # If encoding is requested, convert the frame to JPEG and encode in base64
            _, buffer = imencode(".jpeg", frame)
            return base64.b64encode(buffer)
        return frame
    # Stop the webcam stream
    def halt(self):
        self.is_active = False
        if self.stream_thread.is_alive():
            self.stream_thread.join()
    # Release resources when exiting
    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.cam.release()
# Class to manage the assistant's interactions
class ChatAssistant:
    def __init__(self, model):
        # Create the chain of operations for generating responses
        self.response_chain = self._build_response_chain(model)
    # Answer a prompt using the model and an image
    def respond(self, query, image_data):
        if not query:  # If there's no prompt, do nothing
            return
        print("User Input:", query)
        # Invoke the model with the prompt and the image
        answer = self.response_chain.invoke(
            {"prompt": query, "image_base64": image_data.decode()},
            config={"configurable": {"session_id": "unused"}},
        ).strip()
        print("Assistant Response:", answer)
        if answer:
            self._text_to_speech(answer)  # Convert the response text to speech
    # Convert text to speech using OpenAI's text-to-speech model
    def _text_to_speech(self, text):
        audio_player = PyAudio().open(format=paInt16, channels=1, rate=24000, output=True)
        # Stream the audio response
        with openai.audio.speech.with_streaming_response.create(
            model="tts-1",
            voice="alloy",
            response_format="pcm",
            input=text,
        ) as audio_stream:
            for audio_chunk in audio_stream.iter_bytes(chunk_size=1024):
                audio_player.write(audio_chunk)
        audio_player.close()
    # Build the chain of operations for handling the assistant's responses
    def _build_response_chain(self, model):
        SYSTEM_PROMPT = """
        You are a witty assistant that will use the chat history and the image
        provided by the user to answer their questions.
        Use few words in your answers. Go straight to the point. Do not use any
        emoticons or emojis. Do not ask the user any questions.
        Be friendly and helpful. Show some personality. Do not be too formal.
        """
        # Create the prompt template using LangChain
        template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=SYSTEM_PROMPT),  # System message for setting the assistant's behavior
                MessagesPlaceholder(variable_name="chat_history"),  # Placeholder for chat history
                (
                    "human",  # Define the human message format
                    [
                        {"type": "text", "text": "{prompt}"},  # The text part of the human message
                        {
                            "type": "image_url",
                            "image_url": "data:image/jpeg;base64,{image_base64}",  # The image part of the human message
                        },
                    ],
                ),
            ]
        )
        # Chain the operations: template -> model -> output parser
        operation_chain = template | model | StrOutputParser()
        # Initialize chat message history
        history = ChatMessageHistory()
        return RunnableWithMessageHistory(
            operation_chain,
            lambda _: history,
            input_messages_key="prompt",
            history_messages_key="chat_history",
        )
# Initialize the camera stream
camera_stream = CameraStream().begin()
# Initialize the chat model
# chat_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
chat_model = ChatOpenAI(model="gpt-4o-mini")
# chat_model = ChatOpenAI(model="gpt-4o")
# Create an instance of the chat assistant
assistant_instance = ChatAssistant(chat_model)
# Callback function to process audio input
def process_audio(recognizer, audio_data):
    try:
        # Recognize the speech using Whisper model
        user_query = recognizer.recognize_whisper(audio_data, model="base", language="english")
        assistant_instance.respond(user_query, camera_stream.capture_frame(encode=True))  # Pass the speech and webcam frame to the assistant
    except UnknownValueError:
        print("Could not understand the audio.")
# Initialize the speech recognizer and microphone
speech_recognizer = Recognizer()
mic = Microphone()
with mic as source:
    speech_recognizer.adjust_for_ambient_noise(source)
# Start listening in the background
stop_listening = speech_recognizer.listen_in_background(mic, process_audio)
try:
    while True:
        cv2.imshow("Camera Feed", camera_stream.capture_frame())  # Display the webcam feed
        if cv2.waitKey(1) in [27, ord("q")]:  # Exit on 'ESC' or 'q' key press
            break
finally:
    camera_stream.halt()  # Stop the camera stream
    cv2.destroyAllWindows()  # Close all OpenCV windows
    stop_listening(wait_for_stop=False)  # Stop listening for audio input
# Configuring quantization to load the model in 4-bit precision for efficient memory usage.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16  # Set the computation type to float16 for faster processing.
)
# Importing the Hugging Face pipeline for the image-to-text task.
# Defining the model ID to be used. This model is specifically designed for generating text from images.
model_id = "llava-hf/llava-1.5-7b-hf"
# Initializing the pipeline with the specified model and quantization configuration.
pipe = pipeline("image-to-text", model=model_id, model_kwargs={"quantization_config": quantization_config})
# Setting the maximum number of tokens to be generated by the model.
max_new_tokens = 200
# Creating a FastAPI instance to define the web application.
app = FastAPI()
# Defining a Pydantic model for the input data format.
# The data should include a 'prompt' string and an 'image' which can be any type.
class TextInput(BaseModel):
    prompt: str
    image: Any
# Defining a GET endpoint at the root URL to check the status of the server.
# This will return a message confirming the server is alive and whether a GPU is available.
@app.get("/")
def status_gpu_check() -> dict[str, str]:
    # Check if a GPU is available for PyTorch.
    gpu_msg = "Available" if torch.cuda.is_available()  else "Unavailable"
    # Return a dictionary with status information.
    return {
        "status": "I am ALIVE!",
        "gpu": gpu_msg
    }
# Defining a POST endpoint that generates text based on an input image and a prompt.
# The endpoint expects a JSON object that matches the TextInput format.
@app.post("/generate/")
async def generate_text(data: TextInput) -> dict[str, str]:
    try:
        # Decode the base64 encoded image from the input data.
        byte_img = eval(data.image)
        img = Image.open(io.BytesIO(base64.b64decode(byte_img)))
        print(type(img))  # Debugging: Print the type of the image object.
        # Pass the image and prompt to the pipeline for text generation.
        response = pipe(img, prompt=data.prompt, generate_kwargs={"max_new_tokens": max_new_tokens})
        # Extract the generated text from the model's output.
        model_out = response[0]["generated_text"]
        # Return the generated text as a JSON response.
        return {"generated_text": model_out}
    except Exception as e:
        # If any exception occurs during the process, print debugging information.
        print("This is an EXCEPTION!!!!!!!!!!!!!!!!!")
        print(type(data))
        print(str(e))
        # Raise an HTTP 500 error with the exception message.
        raise HTTPException(status_code=500, detail=str(e))
# This script is designed to start a FastAPI server and monitor its startup time, particularly when the model
# needs to be downloaded, which can take several minutes. The script uses a simple loop to check when the server
# is up and running and displays the elapsed time.
# Initialize an HTML widget to display the elapsed time since the server started.
t = HTML(
    value="0 Seconds",  # Initial value of the timer display.
    description='Server is Starting Up... Elapsed Time:',  # Description displayed next to the timer.
    style={'description_width': 'initial'},  # Style settings for the widget.
)
display(t)  # Display the widget in the notebook.
# Flag to track whether the server has started.
flag = True
# Timer variable to keep track of the elapsed time in seconds.
timer = 0
try:
    # Try to make an HTTP request to the server. If the server is running, it will return a response.
    subprocess.check_output(['curl', "localhost:8000"])
    # If the above command succeeds, the server is already running, so set the flag to False.
    flag = False
except:
    # If the server is not running, start the server using uvicorn in the background.
    # The output of the server is redirected to server.log.
    get_ipython().system_raw('uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &')
# Placeholder for server response.
res = ""
# Loop to continuously check if the server has started. The loop will continue for up to 600 seconds (10 minutes).
while(flag and timer < 600):
    try:
        # Attempt to connect to the server.
        subprocess.check_output(['curl', "localhost:8000"])
    except:
        # If the server is not yet running, wait for 1 second before retrying.
        time.sleep(1)
        # Increment the timer by 1 second.
        timer += 1
        # Update the HTML widget to show the elapsed time.
        t.value = str(timer) + " Seconds"
        # Continue looping until the server starts or the timer exceeds 600 seconds.
        pass
    else:
        # If the server starts successfully, set the flag to False to exit the loop.
        flag = False
# If the server takes more than 10 minutes to start, print an error message.
if(timer >= 600):
    print("Error: timed out! took more than 10 minutes :(")
# Finally, check if the server is running by making a final HTTP request.
subprocess.check_output(['curl', "localhost:8000"])
# This script starts an Ngrok tunnel to expose the local FastAPI server running on port 8000 to the public internet.
# It retrieves the public URL created by Ngrok and stores it for later use.
# Start Ngrok to tunnel the FastAPI server running on localhost:8000.
# The system_raw method is used to run the command in the background without blocking the notebook.
get_ipython().system_raw('ngrok http 8000 &')
# Sleep for 1 second to ensure Ngrok has time to initialize.
time.sleep(1)
# Use curl to query Ngrok's API for the tunnel details. The API is available at localhost:4040.
# The output is returned as a string.
curlOut = subprocess.check_output(['curl', "http://localhost:4040/api/tunnels"], universal_newlines=True)
# Sleep for an additional second to ensure the data is available.
time.sleep(1)
# Parse the JSON output from the curl command to extract the public URL.
ngrokURL = json.loads(curlOut)['tunnels'][0]['public_url']
# Store the public URL in the notebook's environment for use in later cells.
# %store is a magic command in IPython that saves variables so they can be accessed later.
# Print the public URL to the console so the user can see it.
print(ngrokURL)
# Load environment variables from a .env file
load_dotenv()
class CameraStream:
    """Manages the webcam stream, allowing frame retrieval in a thread-safe manner."""
    def __init__(self):
        # Initialize the video capture from the default webcam
        self.capture = VideoCapture(index=0)
        _, self.current_frame = self.capture.read()  # Capture the initial frame
        self.is_active = False
        self.lock = Lock()
    def start(self):
        """Starts the webcam stream in a separate thread."""
        if self.is_active:
            return self
        self.is_active = True
        # Create and start a thread that continuously updates the current frame
        self.thread = Thread(target=self.update_frames, args=())
        self.thread.start()
        return self
    def update_frames(self):
        """Continuously captures frames from the webcam."""
        while self.is_active:
            _, frame = self.capture.read()
            # Update the current frame with a lock to ensure thread safety
            with self.lock:
                self.current_frame = frame
    def get_frame(self, encode=False):
        """Retrieves the current frame, optionally encoding it as a base64 string."""
        with self.lock:
            frame = self.current_frame.copy()
        if encode:
            _, buffer = imencode(".jpeg", frame)
            return base64.b64encode(buffer)
        return frame
    def stop(self):
        """Stops the webcam stream and waits for the thread to finish."""
        self.is_active = False
        if self.thread.is_alive():
            self.thread.join()
    def __exit__(self, exc_type, exc_value, exc_traceback):
        """Releases the video capture when the object is destroyed."""
        self.capture.release()
class SmartAssistant:
    """A class representing the AI assistant that handles image and speech recognition."""
    def __init__(self, api_url):
        # Initialize the chat message history
        self.chat_history = ChatMessageHistory()
        self.api_url = api_url
    def generate_response(self, user_input, image_data):
        """Generates a response based on user input and image data."""
        if not user_input:
            return
        print("User Input:", user_input)
        response_text = self.query_api(user_input, image_data).strip()
        print("Assistant's Response:", response_text)
        if response_text:
            self.text_to_speech(response_text)
    def text_to_speech(self, text):
        """Converts text to speech and plays it using Pygame."""
        # Convert the text response to speech using gTTS
        tts = gTTS(text=text, lang='en')
        with io.BytesIO() as audio_stream:
            tts.write_to_fp(audio_stream)
            audio_stream.seek(0)
            # Initialize Pygame mixer and play the audio from the bytes object
            pygame.mixer.init()
            pygame.mixer.music.load(audio_stream, 'mp3')
            pygame.mixer.music.play()
            # Wait until the audio finishes playing
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
    def query_api(self, user_input, image_data):
        """Sends a request to the AI API with the user input and image data, then returns the response."""
        SYSTEM_PROMPT = """
        You are a witty assistant that will use the chat history and the image
        provided by the user to answer its questions.
        Use few words in your answers. Go straight to the point. Do not use any
        emoticons or emojis. Do not ask the user any questions.
        Be friendly and helpful. Show some personality. Do not be too formal.
        \n
        """
        # Construct the final prompt by combining the system prompt and user input
        combined_prompt = SYSTEM_PROMPT + user_input
        final_prompt = "USER: <image>\n {} \nASSISTANT:".format(combined_prompt)
        # Prepare the data payload for the API request
        request_data = {
            'prompt': final_prompt,
            'image': str(image_data)
        }
        # Send the POST request to the AI API
        response = requests.post(self.api_url + "/generate/", json=request_data)
        if response.status_code == 200:
            result = response.json()
            print('*' * 10)
            print(result["generated_text"].strip())
            print('*' * 10)
            return re.search(r'ASSISTANT:\s*(.*)', (result["generated_text"].strip())).group(1).strip()
        else:
            print("API request failed with status code:", response.status_code)
            return "Let's try that again."
# Initialize the webcam stream and start it
camera_stream = CameraStream().start()
# Initialize the SmartAssistant with the API URL
api_endpoint = "https://86de-35-247-187-146.ngrok-free.app"
assistant = SmartAssistant(api_endpoint)
def process_audio_input(recognizer, audio):
    """Processes the audio input from the microphone."""
    try:
        # Recognize the speech using the Whisper model
        user_input = recognizer.recognize_whisper(audio, model="base", language="english")
        assistant.generate_response(user_input, camera_stream.get_frame(encode=True))
    except UnknownValueError:
        print("There was an error processing the audio.")
# Initialize the speech recognizer and microphone
speech_recognizer = Recognizer()
mic = Microphone()
with mic as source:
    speech_recognizer.adjust_for_ambient_noise(source)
# Start listening in the background with the callback function
stop_listening = speech_recognizer.listen_in_background(mic, process_audio_input)
try:
    # Continuously display the webcam feed
    while True:
        cv2.imshow("Webcam Feed", camera_stream.get_frame())
        if cv2.waitKey(1) in [27, ord("q")]:
            break
finally:
    # Stop the webcam stream and close the window
    camera_stream.stop()
    cv2.destroyAllWindows()
    stop_listening(wait_for_stop=False)

if __name__ == "__main__":
    # Run the main functionality
    print("Notebook code extracted successfully")